{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5825b3-ef40-4c90-9ede-a9b7795fb12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {\n",
       "    margin-left: 0 !important;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {\n",
    "    margin-left: 0 !important;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63390c-6853-43bf-bba9-e12e4e170f6c",
   "metadata": {},
   "source": [
    "# A/B Testing - Frequentist approach\n",
    "A/B testing is a controlled experiment that compares two versions (A and B) to determine which performs better on a chosen metric, such as conversion rate, click-through rate, or revenue.\n",
    "\n",
    "- Key Points of A/B Testing:\n",
    "    - Random assignment: Users or subjects are randomly split into two groups to test variant A versus variant B.\n",
    "    - Hypotheses:\n",
    "        -  Null Hypothesis ($H_0$): There is no difference between A and B.\n",
    "        -  Alternative Hypothesis ($H_1$): There is a difference (e.g., B performs better than A).\n",
    "    - Data collection: Collect data on user interaction with each variant.\n",
    "    - Statistical analysis: Use hypothesis testing (t-tests, z-tests, or nonparametric tests) to determine if observed differences are statistically significant.\n",
    "    - Decision making: If the null hypothesis is rejected with sufficient evidence, conclude that one variant is superior; otherwise, no difference is confirmed.\n",
    "- Practical Steps:\n",
    "    - Formulate hypotheses.\n",
    "    - Randomly assign users to groups.\n",
    "    - Run the experiment, collecting relevant metrics.\n",
    "    - Perform statistical tests to check for significant differences.\n",
    "    - Implement the winning variant based on results.\n",
    "\n",
    "A/B testing is widely used in product development, marketing, and UX research to optimize user engagement and business outcomes by data-driven decision-making. This method is a real-world application of statistical hypothesis testing designed for practical experiments involving two variants.\n",
    "\n",
    "#### Statistical Errors and Power\n",
    "When you run an A/B test, you are making a decision about the whole population based on a sample. Because of sampling variation, there are two primary mistakes (errors) you can make:\n",
    "| Error Type   | Definition                                              | Consequence                                                                                   | Statistical Measure       |\n",
    "|--------------|---------------------------------------------------------|----------------------------------------------------------------------------------------------|---------------------------|\n",
    "| **Type I Error** | False Positive: Rejecting $(H_0)$ when $(H_0)$ is true | Deploying variant B thinking it is better when it is actually no better than control A. Loss of time/resources. | Significance Level $(\\alpha)$ |\n",
    "| **Type II Error** | False Negative: Failing to reject $(H_0)$ when $(H_1)$ is true | Failing to deploy a winning variant B because results were inconclusive. Loss of potential profit. | Statistical Power $(1 - \\beta)$ |\n",
    "\n",
    "##### Frequentist Analysis: P-Value vs. Confidence Interval\n",
    "| Metric           | Purpose                | Output                         | Decision Rule                                |\n",
    "|------------------|------------------------|--------------------------------|----------------------------------------------|\n",
    "| P-value          | Significance (Pass/Fail) | Single number between 0 and 1  | If $(p < \\alpha)$ (usually 0.05), declare a winner reject $(H_0)$. |\n",
    "| Confidence Interval (CI) | Magnitude (How Much?)    | Range of values (e.g., [3.0%, 7.0%]) | If the range excludes zero, the result is significant.              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2847234-f946-4787-9eae-b7d931226a3d",
   "metadata": {},
   "source": [
    "#### 1. Understand the Experiment Design\n",
    "- Clarify the goal: Increase conversion rate (the number of users who pay for the product).\n",
    "- Identify groups: ‚ÄúControl‚Äù (old page) vs. ‚ÄúTreatment‚Äù (new page).\n",
    "- Know your metrics: Focus on conversion rate, but also review related metrics like average order value or bounce rate for context.\n",
    "\n",
    "#### 2. Load and Explore the Data\n",
    "- Import all relevant data files (typically user-level logs or summaries of visits, conversions, variant assignments, etc.).\n",
    "- Use .head(), .info(), and basic plots (hist(), value_counts()) to get a sense for data shape, missingness, and distributions.\n",
    "\n",
    "#### 3. Check Data Integrity\n",
    "- Ensure randomization: Each user should be assigned to only one group.\n",
    "- Check for duplicates and missing values.\n",
    "- Validate that test groups are balanced in size.\n",
    "\n",
    "#### 4. Define Success Criteria\n",
    "- Decide on the statistical significance threshold ($\\alpha$ is standard).\n",
    "- Set the minimum detectable effect (MDE) your business cares about.\n",
    "- Clarify sample size requirements for adequate statistical power.\n",
    "\n",
    "#### 5. Perform Exploratory Data Analysis (EDA)\n",
    "- Visualize conversion rates in control vs. treatment groups.\n",
    "- Plot histograms or boxplots for order value and other key metrics.\n",
    "- Summarize basic statistics: mean, median, counts, proportions.\n",
    "\n",
    "#### 6. Statistical Testing\n",
    "- Calculate the observed difference in conversion rates.\n",
    "- Use appropriate hypothesis tests (e.g., z-test for proportions, t-test if comparing means) to assess significance.\n",
    "- Consider permutation tests if assumptions for parametric tests are questionable.\n",
    "\n",
    "#### 7. Interpret Results\n",
    "- Compare $\\rho-value$ to the $\\alpha$ threshold.\n",
    "- If significant: consider business and practical impact.\n",
    "- If not significant: review sample size and power‚Äîconsider whether to extend testing.\n",
    "\n",
    "#### 8. Additional Testing and Validation Steps\n",
    "- Common Additional Testing and Validation Steps:\n",
    "    - Permutation (Randomization) Tests: Directly estimate the null distribution of your test statistic by permuting group labels. Confirms robustness if assumptions of traditional parametric tests (like normality) are questionable.\n",
    "    - Bootstrap Confidence Intervals: Resample your observed data to estimate more robust confidence intervals for differences in conversion rate or other metrics.\n",
    "    - Subgroup Analysis: Check if effects are consistent across different customer segments (e.g., geography, device type, user tenure) to rule out confounding or strange heterogeneity.\n",
    "    - Test for Balance: Re-validate that the treatment and control groups are similar in covariates prior to treatment‚Äîimbalance could invalidate causal inference.\n",
    "    - Holdout Validation or Split-Test Replication: Run a smaller version of the experiment (or leave out a random subset as a ‚Äúholdout‚Äù group) to check if effects replicate.\n",
    "    - Power Analysis Post-Hoc: Calculate the observed power of your test to help interpret non-significant results‚Äîis the test underpowered, or is there truly no effect?\n",
    "\n",
    "Note: If your main A/B z-test yields a p-value just above 0.05, running a permutation test and bootstrap interval can confirm whether this result is robust or might vary with sampling noise. Subgroup analysis may reveal that the treatment only helps a specific user group‚Äîaffecting your rollout decision.\n",
    "\n",
    "#### 9. Make and Justify Recommendation\n",
    "- Based on the statistical and practical analysis, advise whether to:\n",
    "    - Roll out the new page,\n",
    "    - Keep the old page,\n",
    "    - Or continue/adjust the experiment. \n",
    "\n",
    "\n",
    "#### Summary of Your A/B Testing Workflow\n",
    "| Step                        | Objective                                | Key Tools / Techniques                                                                            |\n",
    "| :-------------------------- | :--------------------------------------- | :------------------------------------------------------------------------------------------------ |\n",
    "| 1. Experiment Design        | Define hypothesis, groups, and metrics.  | Clarify goal (conversion ‚Üë), define control/treatment, metric definitions.                        |\n",
    "| 2. Load & Explore Data      | Understand data structure and quality.    | pandas, `.info()`, `.describe()`, histograms, missing value checks.                              |\n",
    "| 3. Data Integrity Checks    | Ensure randomization and data quality.    | Check duplicates, assignment consistency, group balance.                                         |\n",
    "| 4. Success Criteria         | Define decision thresholds.               | Œ± (e.g., 0.05), Minimum Detectable Effect (MDE), power analysis (`statsmodels.stats.power`).     |\n",
    "| 5. Exploratory Data Analysis (EDA) | Summarize and visualize metrics.   | Conversion rate by group, boxplots, histograms, correlations.                                    |\n",
    "| 6. Statistical Testing      | Evaluate if observed differences are significant. | z-test (proportions), t-test (means), permutation or bootstrap if needed.                |\n",
    "| 7. Interpret Results        | Draw meaningful conclusions.              | Compare p-value vs Œ±, assess practical vs statistical significance.                              |\n",
    "| 8. Validation / Robustness Checks | Strengthen confidence in findings.  | Permutation tests, bootstrapping, subgroup analysis, covariate balance.                          |\n",
    "| 9. Recommendation           | Translate stats into business action.     | Rollout / hold / rerun decision.                                                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efec56-d4ef-4720-936a-663f07e5259d",
   "metadata": {},
   "source": [
    "## What is a Hypothesis?\n",
    "In statistics, a hypothesis is a specific, testable statement about a population parameter (like a mean, proportion, or variance).\n",
    "In A/B testing, it‚Äôs used to decide whether an observed difference between two groups (A and B) is real or just due to random chance.\n",
    "A hypothesis in statistics is a specific, testable statement about a population parameter, such as a mean or proportion. In A/B testing, hypotheses help determine whether an observed difference between groups is real or due to random chance.\n",
    "\n",
    "#### The Two Competing Hypotheses\n",
    "| Type                  | Symbol      | Meaning                                                              |\n",
    "|-----------------------|-------------|----------------------------------------------------------------------|\n",
    "| Null Hypothesis       | $H_0$     | Assumes no difference between A and B. Any observed difference is due to random variation. |\n",
    "| Alternative Hypothesis | $H_1$ or $H_a$ | Assumes there is a real difference (the new variant changed the metric).                |\n",
    "\n",
    "#### Example: A/B Test on Average Order Value (AOV)\n",
    "You test whether a new marketing strategy (B) increases AOV compared to the current one (A).\n",
    "- $H_0:\\mu_A$ = $\\mu_B$ (no difference in mean AOV)\n",
    "- $H_1:\\mu_B$ > $\\mu_A$ (variant B increases mean AOV)\n",
    "\n",
    "#### One-tailed vs Two-tailed Tests\n",
    "| Test Type   | When to Use                                   | Example                             |\n",
    "|-------------|----------------------------------------------|-----------------------------------|\n",
    "| Two-tailed  | When any difference (increase or decrease) is interesting. | $H_1: \\mu_A \\ne \\mu_B$        |\n",
    "| One-tailed  | When you care about only one direction (e.g., increase).   | $H_1: \\mu_B > \\mu_A$           |\n",
    "\n",
    "**‚ö†Ô∏è One-tailed tests are more powerful but risk bias if the effect goes in the opposite direction.**\n",
    "\n",
    "#### Decision Framework\n",
    "| Step | Concept                       | Description                                                       |\n",
    "| :--- | :---------------------------- | :---------------------------------------------------------------- |\n",
    "| 1    | **State hypotheses**          | Define ($H_0$) and ($H_1$).                                       |\n",
    "| 2    | **Choose significance level** | Typically ($\\alpha$ = 0.05).                                      |\n",
    "| 3    | **Compute test statistic**    | e.g., t, z, U ‚Äî depending on test.                                |\n",
    "| 4    | **Compute p-value**           | Probability of seeing your data if ($H_0$) were true.             |\n",
    "| 5    | **Decision rule**             | If ($\\rho \\le \\alpha$), **reject ($H_0$)** ‚Üí significant difference. |\n",
    "\n",
    "#### Example (t-test)\n",
    "```\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.normal(50, 10, 100)\n",
    "B = np.random.normal(52, 10, 100)\n",
    "\n",
    "tstat, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"t = {tstat:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "\n",
    "#### Output example:\n",
    "t = -2.041, p = 0.043\n",
    "\n",
    "**Interpretation:** <br>\n",
    "Since p = 0.043 < 0.05, reject $H_0$ <br>\n",
    "The difference between groups is statistically significant. <br>\n",
    "\n",
    "#### Type I and Type II Errors\n",
    "| Error Type        | Symbol | What It Means                                               | Example                                   |\n",
    "| :---------------- | :----- | :---------------------------------------------------------- | :---------------------------------------- |\n",
    "| **Type I Error**  | $\\alpha$      | Rejecting ($H_0$) when it‚Äôs true (false positive).          | You think B works better, but it doesn‚Äôt. |\n",
    "| **Type II Error** | $\\beta$      | Failing to reject ($H_0$) when it‚Äôs false (false negative). | You miss a real improvement.              |\n",
    "\n",
    "**Power = $1 ‚Äì \\beta$ ‚Üí probability of correctly detecting a real effect.**\n",
    "\n",
    "#### Hypothesis Testing in A/B Context\n",
    "| **Metric**                  | **Null Hypothesis ($H_0$)** | **Alternative ($H_1$)** | **Test Used**         |\n",
    "| :-------------------------- | :-------------------------- | :---------------------- | :-------------------- |\n",
    "| Conversion Rate (CR)        | ($p_A = p_B$)               | ($p_A \\ne p_B$)         | Two-proportion z-test |\n",
    "| Average Order Value (AOV)   | ($\\mu_A = \\mu_B$)           | ($\\mu_A \\ne \\mu_B$)     | t-test                |\n",
    "| Orders per User (Frequency) | ($F_A = F_B$)               | ($F_A \\ne F_B$)         | Mann‚ÄìWhitney U test   |\n",
    "| Revenue per Visitor (RPV)   | ($\\mu_A = \\mu_B$)           | ($%\\mu_A \\ne \\mu_B$)    | t-test                |\n",
    "\n",
    "#### Example Summary (A/B test on AOV)\n",
    "| Step                              | Result                                       |\n",
    "| :-------------------------------- | :------------------------------------------- |\n",
    "| ($H_0$): No difference in AOV     |                                              |\n",
    "| ($H_1$): Strategy B increases AOV |                                              |\n",
    "| $\\alpha = 0.05$                   | Significance threshold                       |\n",
    "| $\\rho = 0.082$                    | Computed $\\rho-value$                        |\n",
    "| Decision: Fail to reject ($H_0$)  | No significant difference detected           |\n",
    "| Interpretation                    | Strategy B didn‚Äôt significantly improve AOV. |\n",
    "\n",
    "#### What is AOV (Average Order Value)?\n",
    "AOV stands for Average Order Value, a key e-commerce and marketing performance metric that measures the average amount of money customers spend per order.\n",
    "\n",
    "**Formula:**\n",
    "$$AVO = \\frac {\\text{Total Revenue}}{\\text{Number of Orders}}$$\n",
    "\n",
    "#### Why AOV Matters\n",
    "| **Business Impact**        | **Explanation**                                                                      |\n",
    "| :------------------------- | :----------------------------------------------------------------------------------- |\n",
    "| **Revenue Growth Lever**   | Increasing AOV can grow total revenue without increasing customer acquisition costs. |\n",
    "| **Pricing & Upselling**    | Measures how well your upsells, bundles, and promotions perform.                     |\n",
    "| **Customer Value Insight** | Helps segment high-value vs. low-value customers.                                    |\n",
    "| **Campaign ROI**           | Determines if a new marketing strategy increases purchase size.                      |\n",
    "\n",
    "#### AOV in A/B Testing\n",
    "In A/B testing, AOV is often used as a primary or secondary metric to assess the financial impact of design or pricing changes.\n",
    "|                                              | **Null Hypothesis ($H_0$)** | **Alternative Hypothesis ($H_1$)** |\n",
    "| :------------------------------------------- | :-------------------------- | :--------------------------------- |\n",
    "| **Goal:** Test if new strategy increases AOV | ($\\mu_A = \\mu_B$)           | ($\\mu_B > \\mu_A$)                  |\n",
    "\n",
    "*Where:*\n",
    "- $\\mu_A$: Mean AOV for Control group (A)\n",
    "- $\\mu_B$: Mean AOV for Variant group (B)\n",
    "\n",
    "#### Statistical Test Used\n",
    "| **Condition**                                      | **Test**                      | **Reason**                                           |\n",
    "| :------------------------------------------------- | :---------------------------- | :--------------------------------------------------- |\n",
    "| Large sample (n > 30/group), AOV roughly symmetric | **Two-sample t-test**         | Tests mean difference assuming approximate normality |\n",
    "| Skewed AOV (common in e-commerce) or small samples | **Mann‚ÄìWhitney U test**       | Non-parametric, does not assume normality            |\n",
    "| Very large samples or bootstrapping available      | **Bootstrap mean difference** | Empirical, assumption-free confidence interval       |\n",
    "```\n",
    "# Sample Code\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example AOVs for A and B\n",
    "A = np.random.normal(50, 15, 1000)\n",
    "B = np.random.normal(52, 15, 1000)\n",
    "\n",
    "# Welch‚Äôs t-test (no equal variance assumption)\n",
    "tstat, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"Mean AOV_A = {A.mean():.2f}, Mean AOV_B = {B.mean():.2f}\")\n",
    "print(f\"t = {tstat:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "**Output:**\n",
    "Mean AOV_A = 50.12, Mean AOV_B = 52.17 <br>\n",
    "t = -2.331, p = 0.020 <br>\n",
    "\n",
    "**‚úÖ Interpretation:**\n",
    "- $\\rho < 0.05$ ‚Üí reject $H_0$\n",
    "- Strategy B significantly increases AOV.\n",
    "\n",
    "#### Common Pitfalls\n",
    "| **Pitfall**                                       | **Why It‚Äôs a Problem**                          | **Better Approach**                                   |\n",
    "| :------------------------------------------------ | :---------------------------------------------- | :---------------------------------------------------- |\n",
    "| AOV is highly skewed (a few huge orders dominate) | Violates t-test assumptions                     | Use **log-transformed AOV** or **Mann‚ÄìWhitney test**  |\n",
    "| Ignoring conversion rate                          | High AOV but low conversions can reduce revenue | Analyze **Revenue per Visitor (RPV = CR √ó AOV)**      |\n",
    "| Comparing cumulative AOV too early                | Early results fluctuate heavily                 | Use **fixed sample window** or **sequential testing** |\n",
    "| Confusing AOV per user vs per order               | Users may place multiple orders                 | Clarify unit of analysis (order-level vs. user-level) |\n",
    "\n",
    "#### Related Metrics\n",
    "| **Metric**                      | **Formula**                               | **Interpretation**                       |\n",
    "| :------------------------------ | :---------------------------------------- | :--------------------------------------- |\n",
    "| **Conversion Rate (CR)**        | ( \\frac{\\text{Orders}}{\\text{Visitors}} ) | % of visitors who buy                    |\n",
    "| **Revenue per Visitor (RPV)**   | ( \\text{CR} \\times \\text{AOV} )           | Average revenue contribution per visitor |\n",
    "| **Orders per User (Frequency)** | ( \\frac{\\text{Orders}}{\\text{Users}} )    | Customer repeat rate                     |\n",
    "\n",
    "#### ‚úÖ Summary\n",
    "| **Aspect**           | **Details**                                              |\n",
    "| :------------------- | :------------------------------------------------------- |\n",
    "| **Definition**       | Mean order value across all transactions                 |\n",
    "| **Goal in A/B test** | Measure if variant increases transaction size            |\n",
    "| **Typical test**     | Welch‚Äôs t-test or Mann‚ÄìWhitney U test                    |\n",
    "| **Business insight** | AOV increase = users buying more expensive or more items |\n",
    "| **Watch for**        | Skewness, low sample size, ignoring CR impact            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4367cb7-4cd0-43cb-b265-d16eb16f8e47",
   "metadata": {},
   "source": [
    "## What is the Normality Assumption?\n",
    "It‚Äôs the assumption that the data (or more precisely, the sampling distribution of the test statistic) follows a normal (Gaussian) distribution. The normality assumption means the data or the sampling distribution of the test statistic follows a normal distribution ($X \\sim N(\\mu, \\sigma^2)$). This assumption underpins many classical tests such as the two-sample t-test used in A/B testing.\n",
    "\n",
    "**In formulas:**\n",
    "- $X \\sim N(\\mu, \\sigma^2)$\n",
    "    - This matters because many classical statistical tests ‚Äî like the t-test ‚Äî are derived under this assumption.\n",
    "\n",
    "#### A/B Testing Context\n",
    "In an A/B test, you compare two versions (A and B) of something (e.g., webpage, email) to see which performs better on a metric (e.g., conversion rate, time spent, click-through rate).\n",
    "\n",
    "Commonly used tests:\n",
    "- Two-sample t-test ‚Äì for comparing means (e.g., average time on site)\n",
    "- Z-test for proportions ‚Äì for comparing rates (e.g., conversion rates)\n",
    "- Nonparametric tests ‚Äì if normality is violated\n",
    "\n",
    "#### When the Normality Assumption Matters\n",
    "| Case                                      | Is Normality Needed? | Why                                                                                                                                                         |\n",
    "| ----------------------------------------- | -------------------- | ---------------------------------------------------------------------------------------------------------------|\n",
    "| **Small sample sizes (n < 30)**           | ‚úÖ Yes                | The t-test assumes data are approximately normal.                                                              |\n",
    "| **Large sample sizes (n ‚â• 30 per group)** | ‚ùå Not strictly       | Thanks to the **Central Limit Theorem (CLT)**, the sampling distribution of the mean (or proportion) becomes approximately normal, even if the data aren‚Äôt. |\n",
    "| **Proportion data (binary outcomes)**     | ‚ùå Not directly       | The Z-test for proportions uses the CLT; normality of the *underlying data* isn‚Äôt required.      |\n",
    "| **Highly skewed data or outliers**        | ‚ö†Ô∏è Maybe not         | Consider data transformation (e.g., log) or a nonparametric alternative.  |\n",
    "\n",
    "**Why Normality Assumption Matters:**\n",
    "- It enables the use of parametric tests with well-understood properties.\n",
    "- When violated with small samples, parametric tests may be invalid.\n",
    "- For large samples, tests remain robust despite non-normality.\n",
    "\n",
    "**Practical Takeaways:**\n",
    "- Check normality when sample sizes are small (tests like Shapiro-Wilk).\n",
    "- For large A/B tests, normality concerns usually diminish.\n",
    "- Use nonparametric tests if normality is violated and sample size is small.\n",
    "This foundation explains why normality remains central in small-sample hypothesis testing but less restrictive for large-scale A/B tests due to the CLT.\n",
    "\n",
    "#### In Practice\n",
    "- For large-scale A/B tests (typical in web experiments), sample sizes are usually huge ‚Üí normality assumption is not a concern.\n",
    "- For small experiments, check normality:\n",
    "    - Visual check (histogram, Q‚ÄìQ plot)\n",
    "    - Statistical tests (Shapiro‚ÄìWilk, Anderson‚ÄìDarling)\n",
    "- If the assumption fails, use:\n",
    "    - Mann-Whitney U test (for medians instead of means)\n",
    "    - Bootstrap methods (nonparametric confidence intervals)\n",
    "\n",
    "#### ‚úÖ Summary\n",
    "| Question                                 | Answer                                                                                       |\n",
    "| ---------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| Do you need normal data for A/B testing? | Usually, **no**, if you have large samples.                                                   |\n",
    "| Why not?                                 | The **Central Limit Theorem** ensures approximate normality of the sample means/proportions. |\n",
    "| When should you care?                    | When your sample is small or the data are extremely skewed.                                  |\n",
    "\n",
    "#### Normality assumption check\n",
    "- Checking the normality assumption is an important step before applying tests like the t-test in A/B testing (especially with small samples).\n",
    "\n",
    "**Step 1. Clarify What You‚Äôre Testing for Normality**\n",
    "- You‚Äôre testing whether your sample data (e.g., metric values from group A and B) are approximately normally distributed.\n",
    "- In A/B testing:\n",
    "    - If your metric is continuous (e.g., time on site, revenue per user) ‚Üí check normality directly.\n",
    "    - If your metric is binary (e.g., converted / not converted) ‚Üí no need to check; the test statistic (proportion) normality comes from the Central Limit Theorem.\n",
    "      \n",
    "**Step 2. Visual Checks**\n",
    "- Histogram\n",
    "    - Plot a histogram of your metric for each group:\n",
    "        - Should look roughly bell-shaped (symmetrical, unimodal).\n",
    "- Q‚ÄìQ (Quantile‚ÄìQuantile) Plot\n",
    "    - Plots your data‚Äôs quantiles vs. those of a theoretical normal distribution:\n",
    "        - If points fall roughly along a $45 \\degree$ line, ‚Üí data are approximately normal.\n",
    "\n",
    "```\n",
    "# Python example:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Suppose `group_a` and `group_b` are arrays or Series of your metric\n",
    "sns.histplot(group_a, kde=True)\n",
    "plt.title(\"Histogram of Group A\")\n",
    "plt.show()\n",
    "\n",
    "stats.probplot(group_a, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot for Group A\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Step 3. Statistical Normality Tests**\n",
    "Formal tests for normality (use with caution ‚Äî they‚Äôre sensitive to large sample sizes):\n",
    "\n",
    "- Shapiro‚ÄìWilk Test\n",
    "    - Most common for small to medium samples (n < 5000).\n",
    "\n",
    "```\n",
    "# Sample Python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(group_a)\n",
    "print(f\"Statistic={stat:.3f}, p={p:.3f}\")\n",
    "if p > 0.05:\n",
    "    print(\"Sample looks normal (fail to reject H‚ÇÄ).\")\n",
    "else:\n",
    "    print(\"Sample does not look normal (reject H‚ÇÄ).\")\n",
    "\n",
    "```\n",
    "\n",
    "- Kolmogorov‚ÄìSmirnov Test or Anderson‚ÄìDarling Test\n",
    "```\n",
    "from scipy.stats import anderson\n",
    "\n",
    "result = anderson(group_a)\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "for i in range(len(result.critical_values)):\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < cv:\n",
    "        print(f\"At {sl}% level: data looks normal.\")\n",
    "    else:\n",
    "        print(f\"At {sl}% level: data not normal.\")\n",
    "\n",
    "```\n",
    "**Step 4. Interpret in Context**\n",
    "- If the data look roughly normal, you can use a t-test.\n",
    "- If not normal, but the sample is large (n ‚â• 30), the Central Limit Theorem justifies approximate normality ‚Üí still okay.\n",
    "- If not normal and small sample, use:\n",
    "    - Mann-Whitney U test (nonparametric)\n",
    "    - Or bootstrap confidence intervals\n",
    "\n",
    "**‚úÖ Summary Table**\n",
    "| Method                | Type        | When to Use | Interpretation                       |\n",
    "| --------------------- | ----------- | ----------- | ------------------------------------ |\n",
    "| Histogram             | Visual      | Always      | Rough shape                          |\n",
    "| Q‚ÄìQ Plot              | Visual      | Always      | Deviation from straight line         |\n",
    "| Shapiro‚ÄìWilk          | Statistical | n < 5000    | p > 0.05 ‚Üí normal                    |\n",
    "| Anderson‚ÄìDarling      | Statistical | Any size    | Compare statistic to critical values |\n",
    "| Large sample (n ‚â• 30) | ‚Äî           | ‚Äî           | Normality assumption not critical    |\n",
    "\n",
    "\n",
    "#### Normality Assumption:\n",
    "- The Normality Assumption is one of the core statistical considerations in A/B testing, especially when you‚Äôre using parametric tests like the t-test or z-test.\n",
    "- In an A/B test, you compare the means of two groups (e.g., control vs. variant).\n",
    "If you‚Äôre using a t-test (e.g., scipy.stats.ttest_ind) or z-test, those tests assume that the sampling distribution of the mean is approximately normal.\n",
    "    - The test assumes that the averages (means) you observe across samples follow a normal (bell-shaped) distribution ‚Äî not necessarily that your raw data are perfectly normal.\n",
    "- Parametric tests like the t-test rely on the Central Limit Theorem (CLT), which says:\n",
    "    - When sample sizes are large enough, the distribution of the sample mean tends to be normal, regardless of the shape of the raw data.\n",
    "    - If your sample size is small, non-normality (e.g., skewed data) can bias your test results.\n",
    "    - If your sample size is large, normality of raw data doesn‚Äôt matter much ‚Äî the CLT protects you.  \n",
    "- Case 1: Small Sample, Non-Normal Data\n",
    "    -  Suppose you have only 30 users per group, and the metric is highly skewed (like revenue per user ‚Äî many zeros, a few large spenders).\n",
    "    - The t-test may not be reliable, because the data are not symmetric. In that case, you‚Äôd prefer a non-parametric test (e.g., Mann‚ÄìWhitney U test).\n",
    "- Case 2: Large Sample (Typical A/B Test)\n",
    "    - If you have 1,000+ users per group:\n",
    "        - Even if your revenue or AOV (Average Order Value) data are skewed, the distribution of the sample mean becomes approximately normal.\n",
    "            - ‚úÖ So you can safely use a t-test or z-test. \n",
    "\n",
    "- $H_0$: The assumption of normal distribution is provided\n",
    "- $H_1$: The assumption of normal distribution is not provided\n",
    "\n",
    "If the p-value is less than 0.05, the test is considered significant, and a nonparametric test (Mann-Whitney U test) will be used. Else, a parametric test (t-test)\n",
    "\n",
    "| Condition              | Data Shape         | Sample Size          | Recommended Test      | Why                                   |\n",
    "|------------------------|--------------------|----------------------|----------------------|--------------------------------------|\n",
    "| Roughly normal data    | Symmetric          | Any                  | t-test               | Meets normality assumption directly  |\n",
    "| Skewed data, small n   | Skewed, heavy tails| < 30‚Äì50 per group    | Mann‚ÄìWhitney U test  | Doesn‚Äôt assume normality             |\n",
    "| Skewed data, large n   | Skewed             | ‚â• 100‚Äì200 per group  | t-test (CLT applies) | Sampling distribution ‚âà normal        |\n",
    "| Proportions (e.g., conversion) | Binary outcome    | Large n              | z-test               | Proportion sampling distribution ‚âà normal |\n",
    "\n",
    "```\n",
    "# Sample Code\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data_A = [50, 55, 52, 70, 120, 30, 45, 50, 48, 52]\n",
    "\n",
    "# Histogram + Q-Q Plot\n",
    "sns.histplot(data_A, kde=True)\n",
    "stats.probplot(data_A, dist=\"norm\", plot=plt) \n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk normality test\n",
    "stat, p = stats.shapiro(data_A)\n",
    "print(f\"Shapiro-Wilk p-value: {p:.4f}\") \n",
    "if p > 0.05:\n",
    "    print(\"‚úÖ Data looks normal (fail to reject H0).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data is likely non-normal (reject H0).\")\n",
    "```\n",
    "#### üßÆ Key Takeaways \n",
    "| ‚úÖ Do‚Äôs                                         | ‚ö†Ô∏è Don‚Äôts                                                         |\n",
    "|------------------------------------------------|------------------------------------------------------------------|\n",
    "| Use t-test if n ‚â• 30 per group (CLT is your friend). | Don‚Äôt assume raw data must be normal ‚Äî it‚Äôs the means that need to be. |\n",
    "| For small or skewed samples, use Mann‚ÄìWhitney U or bootstrap tests. | Don‚Äôt apply t-tests blindly to highly skewed or bounded data (like conversion rates). |\n",
    "| Always visualize distributions (histograms, Q-Q plots). | Don‚Äôt rely only on normality tests for large samples ‚Äî they often flag trivial deviations. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0873a0-ae29-4f84-900b-25251f841778",
   "metadata": {},
   "source": [
    "## Variance Homogeneity\n",
    "One of the key statistical assumptions in A/B testing and t-tests: The assumption of variance homogeneity (also called homoscedasticity).\n",
    "- Variance homogeneity (or equal variance assumption) means\n",
    "    - The spread (variance or standard deviation) of your metric (e.g., AOV, conversion rate, time on site)\n",
    "is roughly the same across all groups being compared.\n",
    "\n",
    "*In A/B testing terms:*\n",
    "- $Var(A) \\approx Var(B)$\n",
    "\n",
    "#### Why It Matters\n",
    "When you run a two-sample t-test, the test formula assumes that both groups have:\n",
    "- Independent samples\n",
    "- Normally distributed means (due to the Central Limit Theorem)\n",
    "- Equal variances (homogeneity)\n",
    "\n",
    "If this assumption is violated:\n",
    "- The standard error of the difference in means is misestimated.\n",
    "- Your $\\rho-values$ and confidence intervals may become inaccurate.\n",
    "\n",
    "**The Two Versions of t-test**\n",
    "| **t-test variant**   | **Assumes equal variances?** | **When to use**                            |\n",
    "| :------------------- | :--------------------------- | :----------------------------------------- |\n",
    "| **Student‚Äôs t-test** | ‚úÖ Yes                        | When variances in both groups are similar  |\n",
    "| **Welch‚Äôs t-test**   | ‚ùå No                         | When variances differ (heteroscedasticity) |\n",
    "\n",
    "**‚úÖ Always safer to use Welch‚Äôs t-test (equal_var=False in Python), since it‚Äôs robust and doesn‚Äôt require equal variances.**\n",
    "\n",
    "```\n",
    "# Example in Python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate two groups\n",
    "np.random.seed(42)\n",
    "A = np.random.normal(50, 10, 500)   # mean=50, sd=10\n",
    "B = np.random.normal(52, 20, 500)   # mean=52, sd=20 (different variance)\n",
    "\n",
    "# Check variances\n",
    "print(f\"Var(A): {np.var(A, ddof=1):.2f}, Var(B): {np.var(B, ddof=1):.2f}\")\n",
    "\n",
    "# Levene's test for equal variances\n",
    "stat, p = stats.levene(A, B)\n",
    "print(f\"Levene‚Äôs test: W = {stat:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# Choose an appropriate t-test\n",
    "if p < 0.05:\n",
    "    print(\"‚ö†Ô∏è Variances differ ‚Äî use Welch‚Äôs t-test (equal_var=False).\")\n",
    "else:\n",
    "    print(\"‚úÖ Variances are similar ‚Äî standard t-test is fine.\")\n",
    "\n",
    "# Welch‚Äôs t-test (default robust option)\n",
    "t, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"Welch‚Äôs t = {t:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "**Output Example** <br>\n",
    "Var(A): 95.92, Var(B): 381.21 <br>\n",
    "Levene‚Äôs test: W = 209.307, p = 0.0000 <br>\n",
    "‚ö†Ô∏è Variances differ ‚Äî use Welch‚Äôs t-test (equal_var=False). <br>\n",
    "Welch‚Äôs t = -2.134, p = 0.0331 <br>\n",
    "\n",
    "**‚úÖ Interpretation:**\n",
    "- The variances are significantly different (p < 0.05 from Levene‚Äôs test).\n",
    "- Therefore, use Welch‚Äôs t-test, which adjusts degrees of freedom and handles unequal variances correctly.\n",
    "\n",
    "#### How to Check Variance Homogeneity\n",
    "| **Test / Method**                      | **Purpose**                                    | **Interpretation**              |\n",
    "| :------------------------------------- | :--------------------------------------------- | :------------------------------ |\n",
    "| **Levene‚Äôs Test** (`stats.levene`)     | Most common; robust to non-normality           | ( p > 0.05 ) ‚Üí variances equal  |\n",
    "| **Bartlett‚Äôs Test** (`stats.bartlett`) | Sensitive to normality; use if data are normal | ( p > 0.05 ) ‚Üí variances equal  |\n",
    "| **Visual Inspection**                  | Boxplots or spread plots                       | Compare spread of data visually |\n",
    "\n",
    "```\n",
    "# Example visualization:\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\"value\": np.concatenate([A, B]),\n",
    "                   \"group\": [\"A\"]*len(A) + [\"B\"]*len(B)})\n",
    "\n",
    "sns.boxplot(data=df, x=\"group\", y=\"value\")\n",
    "plt.title(\"Visual check for variance homogeneity\")\n",
    "plt.show()\n",
    "```\n",
    "#### Summary Table\n",
    "| **Concept**      | **Symbol / Term**                  | **Interpretation**                          |\n",
    "| :--------------- | :--------------------------------- | :------------------------------------------ |\n",
    "| Equal variance   | ($\\sigma^2_A = \\sigma^2_B$)        | Assumed in Student‚Äôs t-test                 |\n",
    "| Unequal variance | ($\\sigma^2_A \\ne \\sigma^2_B$)      | Violates homogeneity                        |\n",
    "| Safe test        | Welch‚Äôs t-test (`equal_var=False`) | Robust to variance differences              |\n",
    "| Check test       | Levene‚Äôs or Bartlett‚Äôs test        | ( p > 0.05 ) ‚Üí OK; ( p < 0.05 ) ‚Üí use Welch |\n",
    "\n",
    "#### Practical Tips in A/B Testing\n",
    "| **Scenario**                                         | **Recommendation**                                                      |\n",
    "| :--------------------------------------------------- | :---------------------------------------------------------------------- |\n",
    "| AOV or revenue metrics (often skewed, high variance) | Use **Welch‚Äôs t-test** by default                                       |\n",
    "| Conversion rate tests                                | Use **proportion z-test** (variance formula known analytically)         |\n",
    "| Small samples with unequal variance                  | Consider **non-parametric test** (Mann‚ÄìWhitney U)                       |\n",
    "| Very large samples                                   | Variance differences have a minor impact, but still report the test type used |\n",
    "\n",
    "**‚úÖ In short:**\n",
    "- Variance homogeneity = equal spread of values between groups.\n",
    "- If violated ‚Üí use Welch‚Äôs t-test.\n",
    "- Always test or visualize before deciding.\n",
    "\n",
    "#### Conceptual Visualization: Variance Homogeneity in t-Tests\n",
    "- We‚Äôd plot two bell curves (the sampling distributions of means for Group A and Group B):\n",
    "1. Equal variance (homoscedastic case):\n",
    "   - Both curves have similar spread (width).\n",
    "   - The t-test assumes this scenario when computing pooled variance.\n",
    "   - The overlap between distributions is symmetrical, so p-values are accurate.\n",
    "2. Unequal variance (heteroscedastic case):\n",
    "   - One curve is much wider (higher variance).\n",
    "   - The assumption of equal spread breaks ‚Äî the standard error is misestimated.\n",
    "   - Student‚Äôs t-test can produce misleading p-values.\n",
    "   - Welch‚Äôs t-test corrects this by using separate variances and adjusted degrees of freedom.\n",
    "\n",
    "| Scenario              | Visualization Idea                         | Interpretation                           |\n",
    "| :-------------------- | :----------------------------------------- | :--------------------------------------- |\n",
    "| **Equal variances**   | Two smooth bell curves with similar widths | ‚úÖ t-test assumption holds                |\n",
    "| **Unequal variances** | One narrow, one wide curve                 | ‚ö†Ô∏è Student‚Äôs t-test invalid; use Welch‚Äôs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d20535",
   "metadata": {},
   "source": [
    "### Strategic & Statistical Robustness Testing\n",
    "These methods check the stability of your result and provide alternative business-friendly metrics.\n",
    "\n",
    "**Bayesian A/B Analysis**\n",
    "- In essence, Bayesian A/B testing allows you to quantify your belief about which variant is better by using probability, instead of just focusing on p-values and confidence intervals.\n",
    "    - Traditional (Frequentist): Asks, \"Assuming the variants are the same (null hypothesis), what is the probability of seeing data as extreme as what we observed?\"\n",
    "    - Bayesian: Asks, \"Given the data we have observed, what is the probability that Variant A is better than Variant B?\"\n",
    "\n",
    "- Key Advantages:\n",
    "    - Incorporates Prior Knowledge: You can formally include what you already know (or believe) about the conversion rate before the test even starts.\n",
    "    - Intuitive Results: The output is a clear, actionable probability (e.g., \"There is a 98% chance that Variant B will generate a higher conversion rate than Variant A\").\n",
    "    - Faster Decision Making: You can often stop the test earlier because the stopping rule is based on reaching a sufficiently high probability of superiority or an acceptable expected loss, rather than a fixed sample size.\n",
    "\n",
    "##### Step-by-Step Bayesian A/B Analysis\n",
    "**1. Define the Prior Distribution**\n",
    "- This is the most \"Bayesian\" step. A prior is a probability distribution representing your belief about the true conversion rate ($\\theta$) of each variant before you see any data.\n",
    "    - Common Choice: The Beta Distribution\n",
    "        - The Beta distribution is the standard choice for modeling probabilities (like conversion rates) because its values are between 0 and 1.\n",
    "        - It has two parameters: $\\alpha$ (the number of successes + 1) and $\\beta$ (the number of failures + 1).\n",
    "    - Choosing your Prior:\n",
    "        - Informative Prior: If you have historical data (e.g., from previous tests), you set $\\alpha$ and $\\beta$ based on those past results.\n",
    "        - Uninformative (Flat) Prior: If you have no prior knowledge, you use $\\text{Beta}(1, 1)$, which assumes all conversion rates between 0% and 100% are equally likely. This is a good starting point.\n",
    "\n",
    "$$\\text{Prior}(\\theta) = \\text{Beta}(\\alpha_{\\text{prior}}, \\beta_{\\text{prior}})$$\n",
    "\n",
    "**2. Collect Data and Apply the Likelihood**\n",
    "Run A/B test and collect the data for each variant (A and B).\n",
    "    - Data for Variant A:\n",
    "        - $N_A$: Total visitors (trials)\n",
    "        - $k_A$: Total conversions (successes)\n",
    "    - Likelihood: The probability of observing $k_A$ successes out of $N_A$ trials, given the true conversion rate $\\theta_A$, is modeled by the Binomial Distribution.\n",
    "\n",
    "**3. Calculate the Posterior Distribution**\n",
    "This is where Bayes' Theorem comes into play. The posterior distribution represents your updated belief about the true conversion rate after seeing the data.\n",
    "- The Math: Thanks to the mathematical convenience of the Beta-Binomial conjugate prior relationship, the posterior distribution is also a Beta Distribution.\n",
    "\n",
    "$$\\text{Posterior}(\\theta) = \\text{Beta}(\\alpha_{\\text{posterior}}, \\beta_{\\text{posterior}})$$\n",
    "\n",
    "- The Update Rule:\n",
    "    - $\\alpha_{\\text{posterior}} = \\alpha_{\\text{prior}} + k$ (prior successes + observed successes)\n",
    "    - $\\beta_{\\text{posterior}} = \\beta_{\\text{prior}} + (N - k)$ (prior failures + observed failures)\n",
    "\n",
    "Calculate a separate posterior distribution for both Variant A and Variant B.\n",
    "\n",
    "**4. Calculate Decision Metrics**<br>\n",
    "Instead of a p-value, you get a distribution for the potential outcome of each variant. You use these distributions to calculate highly actionable metrics:\n",
    "\n",
    "A. Probability of Superiority (PoS)\n",
    "- What it is: The probability that the true conversion rate of one variant (e.g., B) is greater than the true conversion rate of another variant (A).\n",
    "- How to Calculate: You simulate (or sample) thousands of times from both posterior distributions ($\\theta_A$ and $\\theta_B$) and count how often the sampled value for $\\theta_B$ is greater than $\\theta_A$.\n",
    "\n",
    "$$\\text{PoS}(\\text{B} > \\text{A}) = P(\\theta_B > \\theta_A \\, | \\, \\text{Data})$$\n",
    "\n",
    "- Decision: If $\\text{PoS}(\\text{B} > \\text{A})$ is, say, 95% or higher, you have a strong reason to choose B.\n",
    "\n",
    "B. Expected Loss (EL)\n",
    "- What it is: The expected loss if you were to deploy the wrong variant. For example, the expected loss if you choose Variant A, but Variant B is actually better.\n",
    "- Decision: You typically choose the variant that minimizes the expected loss.\n",
    "\n",
    "**5. Decision and Stopping Rule** <br>\n",
    "Unlike frequentist tests which require pre-calculating a sample size, Bayesian tests allow for continuous monitoring (though you should still let it run for a sufficient time, typically one or more full business cycles, to account for time-based variability).\n",
    "- The Rule: Stop the test and declare a winner when the PoS reaches a predetermined threshold (e.g., 95%, 99%) AND the data size is sufficient to reflect typical user behavior (e.g., a week or two). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d660d",
   "metadata": {},
   "source": [
    "Example of comparing two Call-to-Action (CTA) button colors, Variant A (Control) and Variant B (Treatment), using the simple and effective Beta-Binomial approach.\n",
    "\n",
    "Numerical Example: The CTA Button Test\n",
    "| Variant          | Visitors ($N$) | Conversions ($k$) | Observed Conversion Rate ($k/N$) |\n",
    "| :--------------- | :------------- | :---------------- | :------------------------------ |\n",
    "| A (Control)      | 10,000         | 200               | 2.00%                           |\n",
    "| B (Treatment)    | 10,000         | 250               | 2.50%                           |\n",
    "\n",
    "Explanation:\n",
    "- Conversion rate is calculated as $\\frac {\\text {Conversions k}}{\\text{Visitors N}}$\n",
    "- Variant A had 2.00% conversion, Variant B had 2.50%, showing an observed uplift of 0.5 percentage points.\n",
    "- This table summarizes the basic input data for statistical tests in an A/B experiment. Further significance testing would assess if the difference is statistically meaningful\n",
    "\n",
    "**Step 1: Define the Prior Distribution** <br>\n",
    "Use a standard uninformative prior for both variants. This represents having no strong prior belief about the conversion rate before the test starts.\n",
    "- Uninformative Prior: $\\text{Beta}(1, 1)$\n",
    "    - $\\alpha_{\\text{prior}} = 1$\n",
    "    - $\\beta_{\\text{prior}} = 1$\n",
    "\n",
    "**Step 2 & 3: Calculate the Posterior Distribution** <br>\n",
    "Update the prior parameters ($\\alpha$ and $\\beta$) with the observed successes ($k$) and failures ($N-k$) for each variant.\n",
    "- Variant A (Control)\n",
    "    - Observed Successes ($k_A$): 200\n",
    "    - Observed Failures ($N_A - k_A$): $10,000 - 200 = 9,800$\n",
    "    - Posterior A: $\\text{Beta}(\\alpha_{\\text{prior}} + k_A, \\beta_{\\text{prior}} + (N_A - k_A))$\n",
    "\n",
    "$$\\text{Posterior A} = \\text{Beta}(1 + 200, 1 + 9,800) = \\mathbf{\\text{Beta}(201, 9801)}$$\n",
    "\n",
    "- Variant B (Treatment)\n",
    "    - Observed Successes ($k_B$): 250\n",
    "    - Observed Failures ($N_B - k_B$): $10,000 - 250 = 9,750$\n",
    "    - Posterior B: $\\text{Beta}(\\alpha_{\\text{prior}} + k_B, \\beta_{\\text{prior}} + (N_B - k_B))$\n",
    "\n",
    "$$\\text{Posterior B} = \\text{Beta}(1 + 250, 1 + 9,750) = \\mathbf{\\text{Beta}(251, 9751)}$$\n",
    "\n",
    "**Step 4: Calculate Decision Metrics (Probability of Superiority)** <br>\n",
    "Now we have two probability distributions, $\\text{Posterior A}$ and $\\text{Posterior B}$, that quantify our belief about the true conversion rate for each variant. The core task is to determine: $P(\\text{CR}_B > \\text{CR}_A \\, | \\, \\text{Data})$ (i.e., the Probability of Superiority).\n",
    "\n",
    "Since this calculation is complex to do by hand (it requires integrating the two Beta distributions), we use a Monte Carlo simulation (which is what modern A/B testing tools do):\n",
    "1. Simulate: Draw 100,000 random samples from $\\text{Posterior A}$ and 100,000 random samples from $\\text{Posterior B}$. Each sample is a plausible true conversion rate for that variant.\n",
    "2. Compare: For each of the 100,000 pairs, check if the sample from B is greater than the sample from A.\n",
    "3. Count: Calculate the proportion of times B's sample was greater than A's sample.\n",
    "\n",
    "| Comparison                       | Result                        |\n",
    "| :------------------------------ | :------------------------------|\n",
    "| Probability of Superiority       | $\\approx \\mathbf{99.8\\%}$     |\n",
    "| Probability of being Equal/Worse | $\\approx 0.2\\%$               |\n",
    "\n",
    "*Note:\n",
    "Conversion Rate (CR) is the percentage of users who complete a desired action (a \"conversion\") out of the total number of users who had the opportunity to complete that action.*\n",
    "\n",
    "$$\\text{CR} = \\frac{\\text{Number of Conversions}}{\\text{Total Number of Visitors or Trials}} \\times 100$$\n",
    "\n",
    "Examples of Conversions:\n",
    "| Context       | Desired Action (Conversion)                           |\n",
    "| :------------ | :---------------------------------------------------|\n",
    "| E-commerce    | Making a purchase, adding an item to the cart.      |\n",
    "| Lead Generation | Submitting a form, signing up for a newsletter.    |\n",
    "| SaaS/App      | Starting a free trial, completing the onboarding process. |\n",
    "| Content      | Clicking a specific call-to-action (CTA) button, downloading an asset. |\n",
    "\n",
    "\n",
    "**Step 5: Decision**\n",
    "\n",
    "The result is highly actionable:\n",
    "- Based on the data we have collected, there is a 99.8% probability that Variant B (the Treatment) has a higher true conversion rate than Variant A (the Control).\n",
    "- This result is much more intuitive than a frequentist statement like \"The p-value is 0.0001, so we reject the null hypothesis.\" With a PoS of 99.8%, you have extremely high confidence to declare Variant B the winner and roll it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0229b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior A: Beta(201, 9801)\n",
      "Posterior B: Beta(251, 9751)\n",
      "------------------------------\n",
      "Number of Samples: 100000\n",
      "Variant B won in 99126 simulations.\n",
      "Probability of Superiority (PoS): 0.9913 (99.13%)\n",
      "Expected Relative Uplift: 0.2543 (25.43%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- 1. Define the Data ---\n",
    "# Variant A (Control) Data\n",
    "N_A = 10000  # Total Visitors\n",
    "k_A = 200    # Total Conversions\n",
    "\n",
    "# Variant B (Treatment) Data\n",
    "N_B = 10000  # Total Visitors\n",
    "k_B = 250    # Total Conversions\n",
    "\n",
    "# --- 2. Define Priors and Calculate Posteriors ---\n",
    "# Using an uninformative prior: Beta(alpha=1, beta=1)\n",
    "alpha_prior = 1\n",
    "beta_prior = 1\n",
    "\n",
    "# Calculate Posterior Parameters (alpha_posterior = alpha_prior + k)\n",
    "# Posterior A: Beta(201, 9801)\n",
    "alpha_A_post = alpha_prior + k_A\n",
    "beta_A_post = beta_prior + (N_A - k_A)\n",
    "\n",
    "# Posterior B: Beta(251, 9751)\n",
    "alpha_B_post = alpha_prior + k_B\n",
    "beta_B_post = beta_prior + (N_B - k_B)\n",
    "\n",
    "print(f\"Posterior A: Beta({alpha_A_post}, {beta_A_post})\")\n",
    "print(f\"Posterior B: Beta({alpha_B_post}, {beta_B_post})\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 3. Monte Carlo Simulation for Probability of Superiority (PoS) ---\n",
    "# We simulate the true conversion rates by sampling from the posterior distributions.\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "# Sample from the Posterior Beta distributions\n",
    "# Each sample represents a plausible true conversion rate for the variant\n",
    "samples_A = stats.beta.rvs(alpha_A_post, beta_A_post, size=NUM_SAMPLES)\n",
    "samples_B = stats.beta.rvs(alpha_B_post, beta_B_post, size=NUM_SAMPLES)\n",
    "\n",
    "# Compare the samples:\n",
    "# Check in how many simulations the CR of B was greater than the CR of A\n",
    "b_beats_a = (samples_B > samples_A).sum()\n",
    "\n",
    "# Calculate the Probability of Superiority\n",
    "probability_of_superiority = b_beats_a / NUM_SAMPLES\n",
    "\n",
    "# --- 4. Calculate Expected Uplift (Mean of the relative difference) ---\n",
    "# Calculate the percentage difference for each sample pair\n",
    "relative_uplift_samples = (samples_B - samples_A) / samples_A\n",
    "expected_uplift = np.mean(relative_uplift_samples)\n",
    "\n",
    "# --- 5. Output Results ---\n",
    "print(f\"Number of Samples: {NUM_SAMPLES}\")\n",
    "print(f\"Variant B won in {b_beats_a} simulations.\")\n",
    "print(f\"Probability of Superiority (PoS): {probability_of_superiority:.4f} ({probability_of_superiority*100:.2f}%)\")\n",
    "print(f\"Expected Relative Uplift: {expected_uplift:.4f} ({expected_uplift*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb0793",
   "metadata": {},
   "source": [
    "##### Code Explanation\n",
    "1. Libraries: We use numpy for efficient array operations and scipy.stats (specifically stats.beta.rvs) to easily draw random samples from the Beta distribution.\n",
    "2. Posterior Calculation: The core Bayesian update is just adding the successes and failures to the prior $\\alpha$ and $\\beta$ values.\n",
    "3. Sampling: stats.beta.rvs(alpha, beta, size=NUM_SAMPLES) draws 100,000 values. We do this for both A and B.\n",
    "4. Probability of Superiority (PoS):\n",
    "    - (samples_B > samples_A) creates an array of True/False values.\n",
    "    - .sum() counts the True values (where B beat A).\n",
    "    - Dividing this count by NUM_SAMPLES gives the final probability.\n",
    "5. Expected Uplift: This is another powerful metric. It tells you the expected percentage improvement you can anticipate if you deploy Variant B. In this case, it should be close to $(2.5\\% - 2.0\\%) / 2.0\\% = 25\\%$. \n",
    "\n",
    "This code provides the two most critical metrics for a business decision: Confidence (PoS) and Magnitude (Expected Uplift)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c01416",
   "metadata": {},
   "source": [
    "**Bayesian A/B analysis is to understand and calculate the Expected Loss.** <br>\n",
    "This metric directly addresses the business risk associated with an experiment, which is arguably the most actionable part of the Bayesian approach.\n",
    "\n",
    "**Expected Loss (EL)** <br>\n",
    "The Expected Loss is the amount of potential profit you leave on the table if you choose the wrong variant. It quantifies the cost of not picking the true winner.\n",
    "\n",
    "**Why is it Important?** <br>\n",
    "In A/B testing, you usually have two goals:\n",
    "- Confidence: Is Variant B actually better? (Answered by Probability of Superiority, PoS).\n",
    "- Risk: How much would it hurt if I'm wrong? (Answered by Expected Loss, EL).\n",
    "\n",
    "If the PoS is 99%, the decision is easy. But what if the PoS is only 70%? Expected Loss helps you decide if a 70% chance of a big gain is worth the 30% risk of a small loss.\n",
    "\n",
    "**Calculating Expected Loss** <br>\n",
    "We calculate the Expected Loss for each variant if we were to choose it and it turned out to be the loser. We then choose the variant that has the minimum Expected Loss.\n",
    "\n",
    "Using our previous example where Variant B is performing better (has the higher mean conversion rate):\n",
    "- EL (if we choose A): The expected loss if we deploy A, but B is actually the winner. This represents the missed opportunity of choosing B.\n",
    "\n",
    "$$\\text{EL}_{\\text{choose A}} = E[\\max(\\text{CR}_B - \\text{CR}_A, 0) \\mid \\text{Data}]$$\n",
    "\n",
    "- EL (if we choose B): The expected loss if we deploy B, but A is actually the winner. This represents the cost of rolling out the worse version.\n",
    "\n",
    "$$\\text{EL}_{\\text{choose B}} = E[\\max(\\text{CR}_A - \\text{CR}_B, 0) \\mid \\text{Data}]$$\n",
    "\n",
    "Python Code for Expected Loss\n",
    "```\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- Data from previous step ---\n",
    "alpha_A_post = 201\n",
    "beta_A_post = 9801\n",
    "alpha_B_post = 251\n",
    "beta_B_post = 9751\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "# Sample from the Posterior Beta distributions\n",
    "samples_A = stats.beta.rvs(alpha_A_post, beta_A_post, size=NUM_SAMPLES)\n",
    "samples_B = stats.beta.rvs(alpha_B_post, beta_B_post, size=NUM_SAMPLES)\n",
    "\n",
    "# --- Expected Loss Calculation ---\n",
    "\n",
    "# 1. Loss if we choose A (and B is actually better)\n",
    "# The loss is (CR_B - CR_A) only when CR_B > CR_A. Otherwise, the loss is 0.\n",
    "loss_choose_A_samples = np.maximum(samples_B - samples_A, 0)\n",
    "expected_loss_A = np.mean(loss_choose_A_samples)\n",
    "\n",
    "# 2. Loss if we choose B (and A is actually better)\n",
    "# The loss is (CR_A - CR_B) only when CR_A > CR_B. Otherwise, the loss is 0.\n",
    "loss_choose_B_samples = np.maximum(samples_A - samples_B, 0)\n",
    "expected_loss_B = np.mean(loss_choose_B_samples)\n",
    "\n",
    "print(f\"Expected Loss if we choose A (Control): {expected_loss_A:.6f}\")\n",
    "print(f\"Expected Loss if we choose B (Treatment): {expected_loss_B:.6f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb9db12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss if we choose A (Control): 0.005007\n",
      "Expected Loss if we choose B (Treatment): 0.000006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- Data from previous step ---\n",
    "alpha_A_post = 201\n",
    "beta_A_post = 9801\n",
    "alpha_B_post = 251\n",
    "beta_B_post = 9751\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "# Sample from the Posterior Beta distributions\n",
    "samples_A = stats.beta.rvs(alpha_A_post, beta_A_post, size=NUM_SAMPLES)\n",
    "samples_B = stats.beta.rvs(alpha_B_post, beta_B_post, size=NUM_SAMPLES)\n",
    "\n",
    "# --- Expected Loss Calculation ---\n",
    "\n",
    "# 1. Loss if we choose A (and B is actually better)\n",
    "# The loss is (CR_B - CR_A) only when CR_B > CR_A. Otherwise, the loss is 0.\n",
    "loss_choose_A_samples = np.maximum(samples_B - samples_A, 0)\n",
    "expected_loss_A = np.mean(loss_choose_A_samples)\n",
    "\n",
    "# 2. Loss if we choose B (and A is actually better)\n",
    "# The loss is (CR_A - CR_B) only when CR_A > CR_B. Otherwise, the loss is 0.\n",
    "loss_choose_B_samples = np.maximum(samples_A - samples_B, 0)\n",
    "expected_loss_B = np.mean(loss_choose_B_samples)\n",
    "\n",
    "print(f\"Expected Loss if we choose A (Control): {expected_loss_A:.6f}\")\n",
    "print(f\"Expected Loss if we choose B (Treatment): {expected_loss_B:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155fd3a8",
   "metadata": {},
   "source": [
    "**Analysis of Expected Loss Results**\n",
    "| Scenario                       | Metric                     | Value    | Interpretation                                                                                                                             |\n",
    "| :-----------------------------| :--------------------------| :--------| :------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Loss if we choose A (Control) | $\\text{EL}_{\\text{choose A}}$ | 0.005011 | This is the expected size of the mistake if you choose Variant A, given that Variant B is the true winner. It represents 0.5011% of conversion rate, which is the expected long-run average difference missed per visitor if deploying A instead of B.  |\n",
    "| Loss if we choose B (Treatment) | $\\text{EL}_{\\text{choose B}}$ | 0.000006 | This is the expected size of the mistake if you choose Variant B, given that Variant A is the true winner. Nearly zero at 0.0006%, indicating low risk choosing B. |\n",
    "\n",
    "**The Business Decision** <br>\n",
    "The primary goal of using the Expected Loss metric is to choose the variant with the minimum Expected Loss.\n",
    "1. Compare:\n",
    "$$\\text{EL}_{\\text{choose A}} \\ (0.005011) \\gg \\text{EL}_{\\text{choose B}} \\ (0.000006)$$\n",
    "2. Conclusion: The Expected Loss if you choose A is roughly 835 times higher than the Expected Loss if you choose B.\n",
    "\n",
    "Therefore, the decision is clear: Choose Variant B (Treatment).\n",
    "\n",
    "**Contextualizing the Risk**\n",
    "These numbers align perfectly with our earlier finding that the Probability of Superiority for B was $\\approx 99.8\\%$.\n",
    "- The low $\\text{EL}_{\\text{choose B}}$ confirms that the risk of B being worse than A is negligible.\n",
    "- The higher $\\text{EL}_{\\text{choose A}}$ confirms that the cost of sticking with A (the missed opportunity) is substantial and should be avoided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb42c77",
   "metadata": {},
   "source": [
    "#### Informative Priors\n",
    "This is one of the most powerful features of Bayesian A/B testing that sets it apart from frequentist methods.\n",
    "\n",
    "**Informative Priors: Leveraging Historical Data** <br>\n",
    "In our previous example, we used an Uninformative Prior ($\\text{Beta}(1, 1)$), which assumes we know nothing about the conversion rate before the test starts.An Informative Prior is a prior distribution that reflects genuine, existing knowledge about the system you are testing.\n",
    "\n",
    "**How to Construct an Informative Prior** <br>\n",
    "To construct an informative prior using the historical performance of the page, category, or similar test element.\n",
    "1. Gather Historical Data: Look back at a stable period (e.g., the last 3 months) for the specific page or a page with a very similar function.\n",
    "- $N_{\\text{hist}}$: Total historical visitors.\n",
    "- $k_{\\text{hist}}$: Total historical conversions.\n",
    "\n",
    "2. Calculate the Historical Average CR: $\\text{CR}_{\\text{hist}} = k_{\\text{hist}} / N_{\\text{hist}}$.\n",
    "\n",
    "3. Use the Data as a Prior: Use the historical counts to define your new, informative prior distribution for the Control variant (Variant A).\n",
    "$$\\text{Prior}_{\\text{Informative}}(\\theta) = \\text{Beta}(\\alpha_{\\text{hist}}, \\beta_{\\text{hist}})$$\n",
    "- $\\alpha_{\\text{hist}} = k_{\\text{hist}} + 1$\n",
    "- $\\beta_{\\text{hist}} = (N_{\\text{hist}} - k_{\\text{hist}}) + 1$\n",
    "\n",
    "Example: Informative Prior vs. Uninformative Prior\n",
    "- Let's say historically your page has an average conversion rate of 3.0% over 20,000 visitors (600 conversions).\n",
    "\n",
    "| Prior Type          | Œ± (Alpha) | Œ≤ (Beta)                | Formula                       | Effect                                                                                     |\n",
    "| :------------------ | :-------- | :---------------------- | :---------------------------- | :----------------------------------------------------------------------------------------- |\n",
    "| Uninformative (Flat) | 1         | 1                       | ($\\text{Beta}(1, 1)$)         | Little influence on results; requires lots of new data to shift beliefs.                   |\n",
    "| Informative         | 600 + 1   | \\((20000 - 600) + 1\\)   | ($\\mathbf{\\text{Beta}(601, 19401)}$) | Strongly centered around 3.0%; requires very compelling new data to shift the belief.     |\n",
    "\n",
    "**Explanation:**\n",
    "- Uninformative priors treat all outcomes as equally likely, needing lots of data to update beliefs.\n",
    "- Informative priors encode historical knowledge (e.g., 600 successes out of 20,000 trials) and make the model conservative, requiring strong evidence for change.\n",
    "\n",
    "**The Power of the Informative Prior**\n",
    "- Faster Decisions: Because your prior distribution is already quite narrow and centered on a known good rate, you need less new data to confidently distinguish between a good variant and a bad variant. The test converges faster.\n",
    "- Realistic Expectations for Control: The Control variant (A) is modeled as $\\text{Beta}(\\alpha_{\\text{A\\_prior}}, \\beta_{\\text{A\\_prior}})$ rather than the flat $\\text{Beta}(1, 1)$. This prevents temporary \"noise\" from making the control look better or worse than it realistically should be in the first few hours of a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f52749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informative Prior for A: Beta(1501, 48501)\n",
      "Uninformative Prior for A: Beta(1, 1)\n",
      "--------------------------------------------------\n",
      "--- POSTERIORS (After 500 Test Visitors) ---\n",
      "A (Informative Prior): Beta(1509, 48993)\n",
      "A (Uninformative Prior): Beta(9, 493)\n",
      "B (Uninformative Prior): Beta(15, 487)\n",
      "--------------------------------------------------\n",
      "--- RESULT WITH INFORMATIVE PRIOR ---\n",
      "Probability of Superiority (B > A): 0.4689 (46.89%)\n"
     ]
    }
   ],
   "source": [
    "# Python Implementation with Informative Priors\n",
    "# In this example, we assume we have 6 months of stable data for our control page.\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- 1. Define Historical Data for Informative Prior ---\n",
    "# Historical performance of the Control page (Variant A) over a long period\n",
    "N_HISTORICAL = 50000  # Total Historical Visitors\n",
    "k_HISTORICAL = 1500   # Total Historical Conversions (3.0% CR)\n",
    "\n",
    "# Calculate the Informative Prior parameters\n",
    "alpha_A_prior_INF = k_HISTORICAL + 1\n",
    "beta_A_prior_INF = (N_HISTORICAL - k_HISTORICAL) + 1\n",
    "\n",
    "# Define the Uninformative Prior (for comparison)\n",
    "alpha_A_prior_UNIF = 1\n",
    "beta_A_prior_UNIF = 1\n",
    "\n",
    "print(f\"Informative Prior for A: Beta({alpha_A_prior_INF}, {beta_A_prior_INF})\")\n",
    "print(f\"Uninformative Prior for A: Beta({alpha_A_prior_UNIF}, {beta_A_prior_UNIF})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. Define New Test Data (Small Sample Size) ---\n",
    "# We'll use a small sample to show the PRIORS' influence\n",
    "N_TEST = 500\n",
    "k_A_test = 8   # Observed CR = 8/500 = 1.6%\n",
    "k_B_test = 14  # Observed CR = 14/500 = 2.8%\n",
    "\n",
    "\n",
    "# --- 3. Calculate Posteriors (Comparing Priors) ---\n",
    "\n",
    "# Posterior A with INFORMATIVE Prior\n",
    "alpha_A_post_INF = alpha_A_prior_INF + k_A_test\n",
    "beta_A_post_INF = beta_A_prior_INF + (N_TEST - k_A_test)\n",
    "\n",
    "# Posterior A with UNINFORMATIVE Prior\n",
    "alpha_A_post_UNIF = alpha_A_prior_UNIF + k_A_test\n",
    "beta_A_post_UNIF = beta_A_prior_UNIF + (N_TEST - k_A_test)\n",
    "\n",
    "# Posterior B (We'll keep the prior uninformative for the new variant B)\n",
    "alpha_B_post = alpha_A_prior_UNIF + k_B_test\n",
    "beta_B_post = beta_A_prior_UNIF + (N_TEST - k_B_test)\n",
    "\n",
    "print(\"--- POSTERIORS (After 500 Test Visitors) ---\")\n",
    "print(f\"A (Informative Prior): Beta({alpha_A_post_INF}, {beta_A_post_INF})\")\n",
    "print(f\"A (Uninformative Prior): Beta({alpha_A_post_UNIF}, {beta_A_post_UNIF})\")\n",
    "print(f\"B (Uninformative Prior): Beta({alpha_B_post}, {beta_B_post})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 4. Monte Carlo Simulation for PoS (Informative Case) ---\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "# Sample from the two relevant posterior distributions (A-INF vs B)\n",
    "samples_A_inf = stats.beta.rvs(alpha_A_post_INF, beta_A_post_INF, size=NUM_SAMPLES)\n",
    "samples_B = stats.beta.rvs(alpha_B_post, beta_B_post, size=NUM_SAMPLES)\n",
    "\n",
    "# Calculate the Probability of Superiority\n",
    "PoS_B_beats_A_INF = (samples_B > samples_A_inf).sum() / NUM_SAMPLES\n",
    "\n",
    "print(\"--- RESULT WITH INFORMATIVE PRIOR ---\")\n",
    "print(f\"Probability of Superiority (B > A): {PoS_B_beats_A_INF:.4f} ({PoS_B_beats_A_INF*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa460ac",
   "metadata": {},
   "source": [
    "**Bayesian A/B test interpretation** <br>\n",
    "A Bayesian analysis with two priors for variant A and an uninformative prior for B, then updated with 500 visitors. The posteriors:\n",
    "- A (informative): Beta(1509, 48993) ‚Üí mean ‚âà 1509 / 50502 ‚âà 0.0299 (2.99%)\n",
    "- A (uninformative): Beta(9, 493) ‚Üí mean ‚âà 9 / 502 ‚âà 0.0179 (1.79%)\n",
    "- B (uninformative): Beta(15, 487) ‚Üí mean ‚âà 15 / 502 ‚âà 0.0299 (2.99%)\n",
    "- Probability B > A (with informative prior for A): 0.4694\n",
    "- Tiny or no difference: With the informative prior, A‚Äôs posterior mean sits near the historical baseline (~3.0%), and B‚Äôs posterior mean is essentially the same. A probability of superiority of 46.94% is below 50%, which is inconclusive.\n",
    "- Prior matters a lot at N=500: The informative prior (Beta(1501, 48501)) dominates A‚Äôs posterior, pulling it toward ~3.0%. The uninformative prior leaves A more sensitive to the observed data; if A had ~8 conversions (posterior Beta(9, 493)), its mean drops to ~1.8%.\n",
    "- Decision-wise: You don‚Äôt have enough evidence to claim B is better than A. Under most Bayesian decision rules, you‚Äôd keep testing.\n",
    "\n",
    "**Interpretation of the Code's Logic**\n",
    "1. Informative Prior: The Informative Prior for A is dominated by the 50,000 historical data points, centering the belief around the 3.0% long-term rate.\n",
    "2. Test Data: The small test sample shows A performing poorly (1.6%) and B performing well (2.8%).\n",
    "3. Posterior Comparison (Crucial Step):\n",
    "    - A (Uninformative Prior): The flat prior quickly shifts to the observed data, resulting in a posterior centered near 1.6% (8 successes out of 500 is a huge influence on a Beta(1,1) prior). This would likely lead to B being declared the winner quickly.\n",
    "    - A (Informative Prior): The many historical data points \"pull\" the posterior towards the long-term 3.0% rate. The small, noisy 1.6% result is dampened by the historical evidence, resulting in a posterior centered closer to 2.98% (a weighted average of 3.0% and 1.6%).\n",
    "4. Result: Using the informative prior gives a more conservative and reliable result. It essentially tells the system, \"I know the control is usually better than 1.6%, so I need more data to believe this current dip is real before I declare B a winner.\" This reduces the chance of acting on noisy, short-term data.\n",
    "\n",
    "\n",
    "##### Practical recommendations\n",
    "- Define a decision threshold:\n",
    "    - Superiority: Require P(B > A) ‚â• 0.95 (or your business‚Äôs threshold).\n",
    "    - Lift-focused: Require P(p_B ‚àí p_A > L) ‚â• 0.95 for a meaningful lift L (e.g., 0.3 percentage points).\n",
    "- Use a ROPE:\n",
    "    - Define a ‚Äúregion of practical equivalence,‚Äù e.g., |p_B ‚àí p_A| < 0.002, and check P(difference ‚àà ROPE). If high, treat variants as practically the same.\n",
    "- Increase sample size:\n",
    "    - With conversion ~3%, signals are weak at 500 visitors. Plan for several tens of thousands per arm to detect sub‚Äëpercentage‚Äëpoint lifts with high confidence.\n",
    "- Align priors to reality:\n",
    "    - If A‚Äôs historical rate is ~3.07%, your informative prior is consistent. Keep priors transparent and justify their weight (effective sample size).\n",
    "\n",
    "- Report-ready summary: ‚ÄúWith an informative prior for A and 500 visitors, the posterior probability that B outperforms A is 46.94%, which does not meet our decision threshold; we will continue the test until either P(B > A) ‚â• 0.95 or the ROPE probability exceeds 0.8.‚Äù\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fc00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Superiority (B > A): 0.4690\n",
      "Probability difference in ROPE (|B - A| < 0.002): 0.2041\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Define posterior parameters ---\n",
    "# Example: A (informative prior) Beta(1509, 48993), B (uninformative prior) Beta(15, 487)\n",
    "alpha_A, beta_A = 1509, 48993\n",
    "alpha_B, beta_B = 15, 487\n",
    "\n",
    "# --- Sampling ---\n",
    "n_samples = 100000\n",
    "samples_A = np.random.beta(alpha_A, beta_A, n_samples)\n",
    "samples_B = np.random.beta(alpha_B, beta_B, n_samples)\n",
    "\n",
    "# --- Probability of Superiority ---\n",
    "p_superiority = np.mean(samples_B > samples_A)\n",
    "\n",
    "# --- ROPE (Region of Practical Equivalence) ---\n",
    "# Define a practical equivalence threshold, e.g. ¬±0.002 (0.2 percentage points)\n",
    "rope_threshold = 0.002\n",
    "diff = samples_B - samples_A\n",
    "p_rope = np.mean(np.abs(diff) < rope_threshold)\n",
    "\n",
    "print(f\"Probability of Superiority (B > A): {p_superiority:.4f}\")\n",
    "print(f\"Probability difference in ROPE (|B - A| < {rope_threshold}): {p_rope:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc606a",
   "metadata": {},
   "source": [
    "**Bayesian A/B test readout**\n",
    "- Probability of superiority: 0.4701\n",
    "    - Below 0.5 and far from common decision thresholds (e.g., 0.9‚Äì0.95). No evidence B is better.\n",
    "- ROPE probability (|B ‚àí A| < 0.002): 0.2059\n",
    "    - Low. There‚Äôs only ~21% chance the difference is practically negligible within ¬±0.2 percentage points.\n",
    "\n",
    "**ROPE stands for Region of Practical Equivalence.**\n",
    "- It‚Äôs a concept used in Bayesian hypothesis testing to decide whether two parameters (like conversion rates in an A/B test) are practically the same, even if they‚Äôre not mathematically identical.\n",
    "- The ROPE is a small interval around zero (or around the null value) that represents differences too small to matter in practice.\n",
    "- If the posterior distribution of the difference between groups falls mostly inside this interval, you conclude the groups are practically equivalent.\n",
    "\n",
    "**Example in A/B testing**\n",
    "- Suppose your baseline conversion rate is ~3%.\n",
    "- You define a ROPE of ¬±0.002 (¬±0.2 percentage points).\n",
    "- If the posterior probability that $P_B-P_A$ lies within [‚àí0.002, +0.002] is high (say $\\ge$ 0.8), you conclude A and B are practically the same.\n",
    "\n",
    "**Why it matters**\n",
    "- Avoids false positives: You don‚Äôt declare a winner based on tiny, meaningless differences.\n",
    "- Business relevance: It ties statistical decisions to practical impact.\n",
    "- Flexibility: You choose the ROPE width based on what counts as ‚Äúnegligible‚Äù in your context (e.g., ¬±0.1 pp vs ¬±0.5 pp)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AB_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
